#!/usr/bin/env python3
"""
ğŸ‡©ğŸ‡ª German Legal AI - VS Code Local Training
Runs locally in VS Code with Jupyter extension
"""

import subprocess
import sys
import os
import time
from datetime import datetime
import warnings
warnings.filterwarnings("ignore")

print("ğŸ‡©ğŸ‡ª German Legal AI - VS Code Local Training")
print("=" * 60)

def install_packages():
    """Install required packages with conflict resolution."""
    print("ğŸ“¦ Installing packages (this may take a few minutes)...")
    
    # Step 1: Uninstall conflicting packages first
    print("ğŸ§¹ Cleaning up existing packages...")
    cleanup_packages = ["transformers", "datasets", "accelerate", "peft", "bitsandbytes", "torch", "torchvision", "torchaudio"]
    
    for package in cleanup_packages:
        try:
            subprocess.run([sys.executable, "-m", "pip", "uninstall", "-y", package], 
                         capture_output=True, check=False)
        except:
            pass
    
    # Step 2: Install NumPy first (compatible version)
    print("ğŸ“Š Installing NumPy...")
    subprocess.run([sys.executable, "-m", "pip", "install", "numpy==1.24.3"], check=True)
    
    # Step 3: Install PyTorch (CPU version for local development)
    print("ğŸ”¥ Installing PyTorch (CPU)...")
    subprocess.run([sys.executable, "-m", "pip", "install", "torch==2.1.0", "torchvision==0.16.0", "torchaudio==2.1.0", "--index-url", "https://download.pytorch.org/whl/cpu"], check=True)
    
    # Step 4: Install ML packages with compatible versions
    ml_packages = [
        "transformers==4.36.0",  # Updated version that includes Mixtral
        "datasets==2.14.6",
        "accelerate==0.24.1",
        "peft==0.7.1",
        "huggingface_hub==0.19.4",
        "pandas==1.5.3",
        "scikit-learn",
        "tqdm"
    ]
    
    for package in ml_packages:
        print(f"ğŸ“¦ Installing {package}...")
        subprocess.run([sys.executable, "-m", "pip", "install", package], check=True)
        time.sleep(1)
    
    print("âœ… All packages installed successfully!")

def check_environment():
    """Check if all packages are working."""
    try:
        import torch
        import numpy as np
        from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, DataCollatorForLanguageModeling
        from peft import LoraConfig, get_peft_model, TaskType
        from datasets import Dataset
        import pandas as pd
        
        print(f"âœ… NumPy: {np.__version__}")
        print(f"âœ… PyTorch: {torch.__version__}")
        print(f"âœ… CUDA Available: {torch.cuda.is_available()}")
        
        if torch.cuda.is_available():
            print(f"âœ… GPU: {torch.cuda.get_device_name(0)}")
            print(f"âœ… GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f}GB")
        else:
            print("ğŸ’» Running on CPU (suitable for development/testing)")
        
        return True
        
    except Exception as e:
        print(f"âŒ Environment check failed: {e}")
        return False

def create_german_legal_dataset():
    """Create the German legal training dataset."""
    import pandas as pd
    from datasets import Dataset
    
    print("ğŸ“š Creating German legal dataset...")
    
    # Expanded German legal examples
    german_legal_data = [
        {
            "instruction": "Analysiere die rechtlichen Probleme bei einer Ã¼berhÃ¶hten Mietkaution.",
            "input": "Der Vermieter verlangt eine Kaution von 5 Monatsmieten bei einer Wohnung mit 1.200 Euro Monatsmiete.",
            "output": "Die geforderte Kaution von 5 Monatsmieten (6.000 Euro) verstÃ¶ÃŸt gegen Â§ 551 BGB. Nach deutschem Mietrecht ist die Kaution auf maximal 3 Monatsmieten begrenzt. Die rechtlich zulÃ¤ssige HÃ¶chstkaution betrÃ¤gt daher 3.600 Euro. Der Mieter kann die Zahlung der Ã¼berschÃ¼ssigen 2.400 Euro verweigern und bei bereits gezahlter Ãœberschreitung die RÃ¼ckzahlung verlangen. Diese Regelung dient dem Mieterschutz und verhindert Ã¼bermÃ¤ÃŸige finanzielle Belastungen."
        },
        {
            "instruction": "Bewerte die RechtmÃ¤ÃŸigkeit einer KÃ¼ndigung in der verlÃ¤ngerten Probezeit.",
            "input": "Ein Arbeitnehmer wird nach 8 Monaten gekÃ¼ndigt. Im Arbeitsvertrag ist eine Probezeit von 12 Monaten vereinbart.",
            "output": "Die vereinbarte Probezeit von 12 Monaten ist nach Â§ 622 Abs. 3 BGB unzulÃ¤ssig, da die gesetzliche HÃ¶chstdauer von 6 Monaten Ã¼berschritten wird. Die KÃ¼ndigung nach 8 Monaten erfolgte auÃŸerhalb der zulÃ¤ssigen Probezeit. Es gelten daher die regulÃ¤ren KÃ¼ndigungsfristen nach Â§ 622 BGB. Die KÃ¼ndigung kÃ¶nnte unwirksam sein, wenn die ordentliche KÃ¼ndigungsfrist nicht eingehalten wurde. Der Arbeitnehmer sollte rechtliche Schritte prÃ¼fen."
        },
        {
            "instruction": "ErklÃ¤re die GewÃ¤hrleistungsrechte beim Gebrauchtwagenkauf mit versteckten MÃ¤ngeln.",
            "input": "Ein KÃ¤ufer entdeckt nach 3 Wochen einen nicht offenbarten Unfallschaden am gekauften Gebrauchtwagen fÃ¼r 15.000 Euro.",
            "output": "Der KÃ¤ufer hat umfassende GewÃ¤hrleistungsrechte nach Â§Â§ 437 ff. BGB. Bei einem nicht offenbarten Unfallschaden liegt ein erheblicher Sachmangel vor. Der KÃ¤ufer kann zunÃ¤chst NacherfÃ¼llung verlangen (Reparatur oder mangelfreie Ersatzlieferung). Nach erfolglosem Ablauf einer angemessenen Frist stehen Minderung des Kaufpreises oder RÃ¼cktritt vom Vertrag zur Wahl. Bei arglistiger TÃ¤uschung durch den VerkÃ¤ufer ist zusÃ¤tzlich eine Anfechtung nach Â§ 123 BGB mÃ¶glich, wodurch SchadensersatzansprÃ¼che entstehen kÃ¶nnen."
        },
        {
            "instruction": "Analysiere die GeschÃ¤ftsfÃ¼hrerhaftung bei Insolvenzverschleppung in einer GmbH.",
            "input": "Ein GmbH-GeschÃ¤ftsfÃ¼hrer hat trotz erkennbarer Ãœberschuldung seit 2 Monaten weitere Verbindlichkeiten von 50.000 Euro eingegangen, ohne einen Insolvenzantrag zu stellen.",
            "output": "Der GeschÃ¤ftsfÃ¼hrer hat schwerwiegend gegen seine Pflichten nach Â§Â§ 15a, 64 InsO verstoÃŸen. Bei ZahlungsunfÃ¤higkeit oder Ãœberschuldung muss unverzÃ¼glich, spÃ¤testens nach drei Wochen, ein Insolvenzantrag gestellt werden. Durch das Eingehen weiterer Verbindlichkeiten macht er sich nach Â§ 15a Abs. 4 InsO persÃ¶nlich schadensersatzpflichtig gegenÃ¼ber den neuen GlÃ¤ubigern in HÃ¶he von 50.000 Euro. ZusÃ¤tzlich droht eine Haftung nach Â§ 43 GmbHG wegen Verletzung der GeschÃ¤ftsfÃ¼hrerpflichten sowie strafrechtliche Verfolgung wegen Insolvenzverschleppung nach Â§ 15a InsO."
        },
        {
            "instruction": "Bewerte einen Nachbarschaftsstreit wegen nÃ¤chtlicher LÃ¤rmbelÃ¤stigung.",
            "input": "Nachbar A spielt tÃ¤glich von 22:00 bis 24:00 Uhr laute Musik mit 70 dB. Nachbar B kann nicht schlafen und hat bereits mehrfach gebeten aufzuhÃ¶ren.",
            "output": "Nachbar B hat begrÃ¼ndete AnsprÃ¼che nach Â§ 1004 BGB (Beseitigungs- und Unterlassungsanspruch) wegen BeeintrÃ¤chtigung seines Eigentums. Nach den Ã¼blichen Hausordnungen und der Rechtsprechung beginnt die Nachtruhe um 22:00 Uhr. Laute Musik mit 70 dB nach 22:00 Uhr stellt eine unzumutbare StÃ¶rung dar, die Ã¼ber das nach Â§ 906 BGB hinzunehmende MaÃŸ hinausgeht. Trotz der Bitten wurde die StÃ¶rung fortgesetzt. Nachbar B kann die sofortige Unterlassung der nÃ¤chtlichen RuhestÃ¶rung verlangen und bei Fortdauer gerichtlichen Schutz durch einstweilige VerfÃ¼gung suchen. ZusÃ¤tzlich sind mietrechtliche Konsequenzen mÃ¶glich."
        },
        {
            "instruction": "ErklÃ¤re die rechtlichen Grundlagen und Pflichten bei einem Kaufvertrag nach BGB.",
            "input": "Ein Unternehmer mÃ¶chte einen Kaufvertrag fÃ¼r BÃ¼romÃ¶bel im Wert von 25.000 Euro abschlieÃŸen. Welche rechtlichen Aspekte sind zu beachten?",
            "output": "Ein Kaufvertrag nach Â§Â§ 433 ff. BGB erfordert: 1) Einigung Ã¼ber die Kaufsache (genaue Spezifikation der BÃ¼romÃ¶bel), 2) Einigung Ã¼ber den Kaufpreis von 25.000 Euro, 3) Zwei Ã¼bereinstimmende WillenserklÃ¤rungen (Angebot und Annahme). Der VerkÃ¤ufer verpflichtet sich zur Ãœbereignung und mangelfreien Ãœbergabe, der KÃ¤ufer zur Zahlung und Abnahme. Wichtige Aspekte: GewÃ¤hrleistung (Â§Â§ 437 ff. BGB) fÃ¼r 2 Jahre, GefahrÃ¼bergang (Â§ 446 BGB), Lieferzeit, eventuelle RÃ¼cktrittsrechte und AGB-Kontrolle bei Verwendung von GeschÃ¤ftsbedingungen."
        },
        {
            "instruction": "Analysiere die Rechtslage bei einer fristlosen KÃ¼ndigung wegen Arbeitszeitbetrug.",
            "input": "Ein Arbeitnehmer hat Ã¼ber 3 Monate systematisch seine Arbeitszeit um tÃ¤glich 2 Stunden verkÃ¼rzt, obwohl er Vollzeit bezahlt wird. Der Arbeitgeber mÃ¶chte fristlos kÃ¼ndigen.",
            "output": "Eine fristlose KÃ¼ndigung nach Â§ 626 BGB ist hier grundsÃ¤tzlich mÃ¶glich, da systematischer Arbeitszeitbetrug einen wichtigen Grund darstellt. Voraussetzungen sind erfÃ¼llt: 1) Schwere Pflichtverletzung des Arbeitnehmers durch vorsÃ¤tzliche TÃ¤uschung, 2) Unzumutbarkeit der Fortsetzung des ArbeitsverhÃ¤ltnisses fÃ¼r den Arbeitgeber, 3) InteressenabwÃ¤gung fÃ¤llt zugunsten des Arbeitgebers aus, 4) Zwei-Wochen-Frist nach Â§ 626 Abs. 2 BGB ab Kenntnis muss eingehalten werden. Der systematische Betrug Ã¼ber 3 Monate rechtfertigt die sofortige Beendigung ohne Abmahnung. ZusÃ¤tzlich kann Schadensersatz wegen der zu Unrecht gezahlten VergÃ¼tung verlangt werden."
        },
        {
            "instruction": "ErklÃ¤re die VerjÃ¤hrungsfristen verschiedener zivilrechtlicher AnsprÃ¼che.",
            "input": "Nach welchen Fristen verjÃ¤hren AnsprÃ¼che aus KaufvertrÃ¤gen, Delikten und Bereicherung?",
            "output": "Die VerjÃ¤hrungsfristen im deutschen Zivilrecht sind wie folgt geregelt: Die regelmÃ¤ÃŸige VerjÃ¤hrungsfrist betrÃ¤gt nach Â§ 195 BGB drei Jahre ab Ende des Jahres der Entstehung und Kenntnis des Anspruchs. GewÃ¤hrleistungsansprÃ¼che beim Kauf verjÃ¤hren nach Â§ 438 BGB bei beweglichen Sachen in zwei Jahren, bei Bauwerken in fÃ¼nf Jahren. SchadensersatzansprÃ¼che aus unerlaubter Handlung (Delikt) verjÃ¤hren nach Â§ 852 BGB in drei Jahren ab Kenntnis von Schaden und SchÃ¤diger, spÃ¤testens nach 30 Jahren. HerausgabeansprÃ¼che aus Eigentum sind nach Â§ 197 BGB erst nach 30 Jahren verjÃ¤hrt. BereicherungsansprÃ¼che unterliegen der dreijÃ¤hrigen VerjÃ¤hrungsfrist des Â§ 195 BGB."
        }
    ]
    
    def format_instruction_following(example):
        """Format examples for German instruction-following training."""
        if example.get('input', '').strip():
            text = f"### Anweisung:\n{example['instruction']}\n\n### Eingabe:\n{example['input']}\n\n### Antwort:\n{example['output']}<|endoftext|>"
        else:
            text = f"### Anweisung:\n{example['instruction']}\n\n### Antwort:\n{example['output']}<|endoftext|>"
        return {"text": text}
    
    # Create dataset
    df = pd.DataFrame(german_legal_data)
    dataset = Dataset.from_pandas(df)
    dataset = dataset.map(format_instruction_following)
    
    # Split for training and validation
    dataset_split = dataset.train_test_split(test_size=0.2, seed=42)
    train_dataset = dataset_split['train']
    eval_dataset = dataset_split['test']
    
    print(f"âœ… Dataset created: {len(train_dataset)} training, {len(eval_dataset)} validation examples")
    print(f"ğŸ“ Sample text preview:")
    print(train_dataset[0]['text'][:400] + "...")
    
    return train_dataset, eval_dataset

def load_model_and_tokenizer():
    """Load a suitable German model and tokenizer."""
    import torch
    from transformers import AutoTokenizer, AutoModelForCausalLM
    
    print("ğŸ¤– Loading model and tokenizer...")
    
    # Use a reliable, smaller German model for local training
    model_options = [
        'microsoft/DialoGPT-medium',  # Fallback - works reliably
        'dbmdz/german-gpt2',         # German-specific GPT-2
        'malteos/gpt2-wechsel-german' # Another German GPT-2
    ]
    
    model_name = None
    tokenizer = None
    
    for option in model_options:
        try:
            print(f"Trying {option}...")
            tokenizer = AutoTokenizer.from_pretrained(option)
            model_name = option
            print(f"âœ… Successfully loaded tokenizer: {model_name}")
            break
        except Exception as e:
            print(f"âš ï¸ Failed to load {option}: {str(e)[:50]}...")
            continue
    
    if not model_name:
        raise Exception("Could not load any suitable model")
    
    # Configure tokenizer
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    tokenizer.padding_side = "right"
    
    # Load model (CPU version for local development)
    print(f"ğŸ“¥ Loading model: {model_name}")
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        torch_dtype=torch.float32,  # Use float32 for CPU
        low_cpu_mem_usage=True,
        trust_remote_code=True
    )
    
    # Enable gradient checkpointing for memory efficiency
    if hasattr(model, 'gradient_checkpointing_enable'):
        model.gradient_checkpointing_enable()
    
    print(f"âœ… Model loaded successfully!")
    print(f"ğŸ”¢ Parameters: {model.num_parameters():,}")
    
    return model, tokenizer, model_name

def setup_lora_training(model, model_name):
    """Configure LoRA for efficient fine-tuning."""
    from peft import LoraConfig, get_peft_model, TaskType
    
    print("ğŸ›ï¸ Setting up LoRA configuration...")
    
    # Determine target modules based on model architecture
    if "gpt2" in model_name.lower() or "dialo" in model_name.lower():
        target_modules = ["c_attn", "c_proj", "c_fc"]
        print("ğŸ”§ Using GPT-2 style modules")
    else:
        # Default transformer modules
        target_modules = ["q_proj", "v_proj", "k_proj", "o_proj"]
        print("ğŸ”§ Using standard transformer modules")
    
    # LoRA configuration - conservative settings for stable training
    lora_config = LoraConfig(
        task_type=TaskType.CAUSAL_LM,
        r=16,                    # LoRA rank
        lora_alpha=32,           # LoRA scaling parameter
        lora_dropout=0.1,        # Dropout for regularization
        target_modules=target_modules,
        bias="none",
    )
    
    # Apply LoRA to the model
    model = get_peft_model(model, lora_config)
    
    # Print trainable parameter statistics
    def print_trainable_parameters(model):
        trainable_params = 0
        all_params = 0
        for _, param in model.named_parameters():
            all_params += param.numel()
            if param.requires_grad:
                trainable_params += param.numel()
        
        trainable_percentage = 100 * trainable_params / all_params
        print(f"ğŸ¯ Trainable parameters: {trainable_params:,}")
        print(f"ğŸ”¢ All parameters: {all_params:,}")
        print(f"ğŸ“Š Trainable percentage: {trainable_percentage:.2f}%")
    
    print_trainable_parameters(model)
    return model

def train_german_legal_model(model, tokenizer, train_dataset, eval_dataset):
    """Train the German legal AI model."""
    import torch
    from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling
    
    print("ğŸ‹ï¸ Setting up training configuration...")
    
    # Tokenization function
    def tokenize_function(examples):
        return tokenizer(
            examples["text"],
            truncation=True,
            padding=False,
            max_length=512,  # Reduced for local training
            return_overflowing_tokens=False,
        )
    
    # Tokenize datasets
    print("ğŸ”¤ Tokenizing training data...")
    tokenized_train = train_dataset.map(
        tokenize_function,
        batched=True,
        remove_columns=train_dataset.column_names,
    )
    
    tokenized_eval = eval_dataset.map(
        tokenize_function,
        batched=True,
        remove_columns=eval_dataset.column_names,
    )
    
    print(f"âœ… Tokenization complete: {len(tokenized_train)} train, {len(tokenized_eval)} eval")
    
    # Training configuration optimized for local CPU/GPU
    output_dir = "./german-legal-model"
    training_args = TrainingArguments(
        output_dir=output_dir,
        overwrite_output_dir=True,
        
        # Training parameters
        num_train_epochs=3,                    # More epochs for better learning
        per_device_train_batch_size=2,        # Small batch for local training
        per_device_eval_batch_size=2,
        gradient_accumulation_steps=4,        # Effective batch size: 8
        learning_rate=5e-5,
        lr_scheduler_type="cosine",
        warmup_ratio=0.1,
        weight_decay=0.01,
        
        # Memory and performance
        fp16=torch.cuda.is_available(),       # Use fp16 only if GPU available
        gradient_checkpointing=True,
        dataloader_pin_memory=False,
        
        # Logging and evaluation
        logging_steps=10,
        eval_steps=25,
        evaluation_strategy="steps",
        save_steps=50,
        save_total_limit=3,
        load_best_model_at_end=True,
        metric_for_best_model="eval_loss",
        greater_is_better=False,
        
        # Other settings
        remove_unused_columns=False,
        report_to=None,  # Disable wandb/tensorboard
        run_name=f"german-legal-{datetime.now().strftime('%Y%m%d-%H%M')}",
        max_grad_norm=1.0,  # Gradient clipping
    )
    
    # Data collator for language modeling
    data_collator = DataCollatorForLanguageModeling(
        tokenizer=tokenizer,
        mlm=False,  # Causal language modeling, not masked
    )
    
    # Initialize trainer
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=tokenized_train,
        eval_dataset=tokenized_eval,
        data_collator=data_collator,
        tokenizer=tokenizer,
    )
    
    # Start training
    print(f"ğŸš€ Starting training...")
    print(f"â±ï¸ Estimated time: {len(tokenized_train) * 3 // 8} minutes")
    
    start_time = time.time()
    
    try:
        # Clear memory cache if CUDA is available
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        
        # Train the model
        training_result = trainer.train()
        
        # Calculate training time
        training_time = time.time() - start_time
        
        print(f"\nâœ… Training completed successfully!")
        print(f"â±ï¸ Training time: {training_time/60:.1f} minutes")
        print(f"ğŸ“Š Final training loss: {training_result.training_loss:.4f}")
        
        # Save the model
        trainer.save_model()
        tokenizer.save_pretrained(output_dir)
        
        print(f"ğŸ’¾ Model saved to: {output_dir}")
        
        return trainer, output_dir
        
    except Exception as e:
        print(f"âŒ Training failed: {str(e)}")
        
        # Try to save partial progress
        try:
            partial_dir = f"{output_dir}-partial"
            trainer.save_model(partial_dir)
            tokenizer.save_pretrained(partial_dir)
            print(f"ğŸ’¾ Partial model saved to: {partial_dir}")
        except:
            pass
        
        raise e

def test_trained_model(model, tokenizer):
    """Test the trained German legal AI model."""
    import torch
    
    print("ğŸ§ª Testing the trained model...")
    
    def generate_legal_response(instruction, input_text="", max_new_tokens=200):
        """Generate a response for a German legal question."""
        # Format the prompt
        if input_text.strip():
            prompt = f"### Anweisung:\n{instruction}\n\n### Eingabe:\n{input_text}\n\n### Antwort:\n"
        else:
            prompt = f"### Anweisung:\n{instruction}\n\n### Antwort:\n"
        
        # Tokenize the prompt
        inputs = tokenizer(prompt, return_tensors="pt")
        
        # Move to same device as model
        if torch.cuda.is_available() and next(model.parameters()).is_cuda:
            inputs = inputs.to("cuda")
        
        # Generate response
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=max_new_tokens,
                temperature=0.7,
                do_sample=True,
                top_p=0.9,
                repetition_penalty=1.1,
                pad_token_id=tokenizer.eos_token_id,
                eos_token_id=tokenizer.eos_token_id,
            )
        
        # Decode the response
        full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)
        response = full_response[len(prompt):].strip()
        
        return response
    
    # Test cases covering various German legal scenarios
    test_cases = [
        {
            "instruction": "ErklÃ¤re die rechtlichen Probleme bei einer Ã¼berhÃ¶hten Mietkaution.",
            "input": "Ein Vermieter fordert 4 Monatsmieten als Kaution fÃ¼r eine 900 Euro Wohnung."
        },
        {
            "instruction": "Bewerte eine KÃ¼ndigung wÃ¤hrend der Probezeit.",
            "input": "Arbeitnehmer wird nach 10 Monaten gekÃ¼ndigt, Probezeit war auf 12 Monate vereinbart."
        },
        {
            "instruction": "Was sind die wichtigsten Rechte eines KÃ¤ufers bei einem mangelhaften Auto?",
            "input": ""
        },
        {
            "instruction": "Analysiere einen Nachbarschaftsstreit wegen LÃ¤rmbelÃ¤stigung.",
            "input": "Nachbar spielt jeden Abend ab 23 Uhr laute Musik fÃ¼r 2 Stunden."
        }
    ]
    
    print("\n" + "="*60)
    print("ğŸ” GERMAN LEGAL AI MODEL TEST RESULTS")
    print("="*60)
    
    for i, test_case in enumerate(test_cases, 1):
        print(f"\nğŸ” Test {i}:")
        print(f"Anweisung: {test_case['instruction']}")
        if test_case['input']:
            print(f"Eingabe: {test_case['input']}")
        
        try:
            response = generate_legal_response(test_case['instruction'], test_case['input'])
            print(f"\nğŸ“ Antwort: {response[:400]}{'...' if len(response) > 400 else ''}")
        except Exception as e:
            print(f"\nâŒ Error generating response: {str(e)[:100]}...")
        
        print("-" * 60)
    
    print("\nâœ… Model testing completed!")

def create_deployment_package(output_dir, model_name):
    """Create a deployment package for the trained model."""
    import json
    import zipfile
    import shutil
    
    print("ğŸ“¦ Creating deployment package...")
    
    if not os.path.exists(output_dir):
        print(f"âŒ Model directory {output_dir} not found")
        return None
    
    # Create export directory
    export_dir = "german-legal-deployment"
    if os.path.exists(export_dir):
        shutil.rmtree(export_dir)
    
    os.makedirs(export_dir)
    
    # Copy model files
    shutil.copytree(output_dir, f"{export_dir}/model")
    
    # Create configuration file
    config = {
        "model_info": {
            "name": "German Legal AI Assistant",
            "base_model": model_name,
            "type": "causal-lm-lora",
            "version": "1.0.0",
            "training_date": datetime.now().isoformat(),
            "language": "German",
            "domain": "Legal"
        },
        "training_config": {
            "epochs": 3,
            "batch_size": 2,
            "learning_rate": 5e-5,
            "lora_r": 16,
            "lora_alpha": 32,
            "max_length": 512
        },
        "inference_config": {
            "max_new_tokens": 256,
            "temperature": 0.7,
            "top_p": 0.9,
            "repetition_penalty": 1.1
        },
        "usage_instructions": {
            "prompt_format": "### Anweisung:\\n{instruction}\\n\\n### Eingabe:\\n{input}\\n\\n### Antwort:\\n",
            "example": "### Anweisung:\\nErklÃ¤re die rechtlichen Aspekte einer Mietkaution.\\n\\n### Antwort:\\n"
        }
    }
    
    with open(f"{export_dir}/config.json", "w", encoding="utf-8") as f:
        json.dump(config, f, indent=2, ensure_ascii=False)
    
    # Create README file
    readme_content = f"""# German Legal AI Model

## Overview
This is a fine-tuned German Legal AI assistant trained on German legal scenarios.

**Model Information:**
- Base Model: {model_name}
- Training Date: {datetime.now().strftime('%Y-%m-%d %H:%M')}
- Language: German
- Domain: Legal advice and analysis

## Usage

### Loading the Model
```python
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel

# Load base model
base_model = AutoModelForCausalLM.from_pretrained("{model_name}")
tokenizer = AutoTokenizer.from_pretrained("./model")

# Load fine-tuned model
model = PeftModel.from_pretrained(base_model, "./model")
```

### Inference
```python
def generate_legal_advice(instruction, input_text=""):
    prompt = f"### Anweisung:\\n{{instruction}}\\n\\n### Eingabe:\\n{{input_text}}\\n\\n### Antwort:\\n"
    inputs = tokenizer(prompt, return_tensors="pt")
    outputs = model.generate(**inputs, max_new_tokens=256)
    return tokenizer.decode(outputs[0], skip_special_tokens=True)
```

## Deployment on AX102 Server
1. Extract this package to your server
2. Install required dependencies: transformers, peft, torch
3. Use the provided configuration for optimal performance
4. Integrate with your existing Docker setup

## Legal Disclaimer
This AI model provides general legal information for educational purposes only.
It should not be considered as professional legal advice.
Always consult with qualified legal professionals for specific legal matters.
"""
    
    with open(f"{export_dir}/README.md", "w", encoding="utf-8") as f:
        f.write(readme_content)
    
    # Create ZIP file
    timestamp = datetime.now().strftime('%Y%m%d-%H%M%S')
    zip_filename = f"german-legal-ai-{timestamp}.zip"
    
    with zipfile.ZipFile(zip_filename, 'w', zipfile.ZIP_DEFLATED) as zipf:
        for root, dirs, files in os.walk(export_dir):
            for file in files:
                file_path = os.path.join(root, file)
                arcname = os.path.relpath(file_path, export_dir)
                zipf.write(file_path, arcname)
    
    print(f"âœ… Deployment package created: {zip_filename}")
    print(f"ğŸ“ Contents:")
    print(f"   ğŸ“‚ model/ - Trained model files")
    print(f"   ğŸ“„ config.json - Configuration and metadata")
    print(f"   ğŸ“– README.md - Usage instructions")
    
    return zip_filename

def main():
    """Main training pipeline for German Legal AI."""
    total_start_time = time.time()
    
    try:
        print("ğŸš€ Starting German Legal AI Training Pipeline")
        print(f"ğŸ“… Started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        
        # Step 1: Install required packages
        print("\n" + "="*60)
        install_packages()
        
        # Step 2: Verify environment
        print("\n" + "="*60)
        print("ğŸ” Verifying environment...")
        if not check_environment():
            print("âŒ Environment verification failed. Please check the installation.")
            return
        
        # Step 3: Create training dataset
        print("\n" + "="*60)
        train_dataset, eval_dataset = create_german_legal_dataset()
        
        # Step 4: Load model and tokenizer
        print("\n" + "="*60)
        model, tokenizer, model_name = load_model_and_tokenizer()
        
        # Step 5: Setup LoRA training
        print("\n" + "="*60)
        model = setup_lora_training(model, model_name)
        
        # Step 6: Train the model
        print("\n" + "="*60)
        trainer, output_dir = train_german_legal_model(model, tokenizer, train_dataset, eval_dataset)
        
        # Step 7: Test the trained model
        print("\n" + "="*60)
        test_trained_model(model, tokenizer)
        
        # Step 8: Create deployment package
        print("\n" + "="*60)
        zip_filename = create_deployment_package(output_dir, model_name)
        
        # Final summary
        total_time = time.time() - total_start_time
        print("\n" + "="*60)
        print("ğŸ‰ GERMAN LEGAL AI TRAINING COMPLETED SUCCESSFULLY!")
        print("="*60)
        print(f"â±ï¸ Total time: {total_time/60:.1f} minutes")
        print(f"ğŸ“¦ Deployment package: {zip_filename}")
        print(f"ğŸ“ Model directory: {output_dir}")
        print(f"ğŸš€ Ready for deployment on your AX102 server!")
        
        print(f"\nğŸ“‹ Next steps:")
        print(f"1. ğŸ“¥ Download/copy {zip_filename} to your AX102 server")
        print(f"2. ğŸ“‚ Extract the package to /opt/german-legal-ai/models/")
        print(f"3. ğŸ”§ Update your Docker configuration to use the new model")
        print(f"4. ğŸš€ Start your services with: ./start.sh")
        
    except Exception as e:
        print(f"\nâŒ Training failed with error: {str(e)}")
        print(f"ğŸ’¡ Check the error message above and try again")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()